{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Truxnell's Home-Cluster","text":"<p>This git repo is my homelab, stored in a declaritve yaml format.  This allows me to have a single source of written truth for my homelab, declaring how and where I want it setup.</p> <p>This allows me to:</p> <ul> <li>Version control my changes, allowing easy rollback of breaking patches/tinkering/etc</li> <li>Allow for easy reinstall/disaster recovery of a cluster, as everything except persistent data is defined here.</li> <li>Version control and declare hardware provisioning (Now using Sidero &amp; Talos), ensuring repeatable and robust hardware configuration.</li> <li>This can be achieved with tools such as Terraform and Ansible for those wanting to use a more standard OS &amp; deployment.</li> <li>With Sidero and Talos, I can define and provision a cluster by plugging nodes into the network, and having them network PXE boot, install the OS Talos, and have a configuration file applied to them.  This automates and watches my cluster, with no manual intervention required.</li> </ul>"},{"location":"#tech-stack","title":"Tech Stack","text":""},{"location":"play/","title":"Play","text":"<p>k8s PV PVC</p> .browserslistrc<pre><code>--8&lt;--\u200b \"index.md\"\n</code></pre>"},{"location":"apps/k10/","title":"Application - k10","text":""},{"location":"apps/k10/#intro","title":"Intro","text":"<p>Kasten k10 is a proprietary backup solution for k8s from Veeam.  Whilst it is paid software, a free license is available for small clusters.</p> <p>It has benefits over other solutions such as:</p> <ul> <li>While it is configured in UI, config is stored in the k8s clusters in CRD's.  These can be exported to yaml and stored in your git, allowing for declarative gitops of settings</li> <li>The ability/concept to do a snapshot, and a export (ex. NFS or S3).  This can allow for example, a hourly snapshot of a game server and nightly export of a snapshot to NFS</li> <li>Storing a set of backups - v Hourly, x daily, y Weekly, z Yearly.</li> <li>Disaster recovery policy which (by default) runs hourly.  This can restore k10 from a blank slate from a external source (NFS/S3), which then allows a full/selecting cluster restore</li> <li>Restore only data pvc's on restore (excellent for us with cluster yaml stored in git)</li> <li>Restore to different namespace (except for cloning a pvc for testing in a test namespace)</li> <li>Comes with its own prometheus and grafana (which can be federated to a main prometheus, as I have done)</li> <li>Clearly shows pvc's or apps that are missed from a backup</li> </ul> <p>k10 Saves</p> <p>This app has already saved my cluster once (at time of writing!).  Don't skimp on backups!</p>"},{"location":"apps/k10/#folder-layout","title":"Folder Layout","text":"<p>This deployment has two layers - the root <code>kustomization.yaml</code> installing the helm-release and disaster recovery secret, and the k10-config folder <code>kustomzation.yaml</code>, with a dependency on the base kustomize.</p> <pre><code>\u251c\u2500\u2500 helmrelease.yaml                        # (1)\n\u251c\u2500\u2500 k10-config                              # (2)\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 blueprints                          # (3)\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 hyperion.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 k10-disaster-recovery.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 kustomization.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 postgresql-blueprint.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 secret.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 kustomization.yaml                  # (4)\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 monitoring                          # (5)\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 kustomization.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 prometheus-rule.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 service-monitor.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 policies                            # (6)\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 daily-backup-policy.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 games-hourly-policy.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 k10-dr-policy.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 kustomization.yaml\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 profiles                            # (7)\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 backblaze-b2-secret.sops.yaml\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 backblaze-b2.yaml\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 home.yaml\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 k10-backups-pvc.yaml\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 k10-disaster-recovery.yaml\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 kustomization.yaml\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 media.yaml\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 synology.yaml\n\u251c\u2500\u2500 kustomization.yaml                      # (8)\n\u2514\u2500\u2500 secret.sops.yaml                        # (9)\n</code></pre> <ol> <li>k10 helmrelease (installs crd's for the config folder)</li> <li>Config folder - loads second after k10 base kustomize has loaded k10 and settled (due to the config being CRD's)</li> <li>Blueprints (allows customization of backup behavior, or pre/post hooks)</li> <li>k10-config kustomization (calls on the 4 folders in this directory)</li> <li>Prometheus monitoring (alert for failed backups etc)</li> <li>Policies (Snapshot &amp; Backup plans)</li> <li>Profiles (Backup locations)</li> <li>Base kustomization</li> <li>SOPS secret for disaster recovery (inc cluster-id key for safekeeping)</li> </ol>"},{"location":"apps/k10/#install","title":"Install","text":"<p>I have taken the route of:</p> <ul> <li>Installing the helm release</li> <li>Setting up with the UI</li> <li>Using the nifty UI helpers that let you copy a <code>kubectl</code> command to view most everything you can configure</li> <li>Putting these into YAML in my github and letting helm reconcile over the top of it.</li> </ul> <p>Gitops risk</p> <p>Having flux reconcile the config does mean any changes you make require putting back into your gitops ASAP to avoid flux reverting all your changes.  This approach has pros/cons.</p> <p>This is only a example for explanation purposes</p> <p>Check my (or others clusters) for up to date examples More helm values info can be found at https://docs.kasten.io/latest/install/advanced.html</p> <pre><code>---\napiVersion: helm.toolkit.fluxcd.io/v2beta1\nkind: HelmRelease\nmetadata:\nname: k10\nnamespace: kasten-io\nspec:\nreleaseName: k10\ninterval: 5m\nchart:\nspec:\nchart: k10\nversion: 4.5.12\nsourceRef:\nkind: HelmRepository\nname: kasten-charts\nnamespace: flux-system\ninterval: 5m\ninstall:\ncreateNamespace: true\ncreateNamespace: true\ncrds: CreateReplace\nremediation:\nretries: 3\nupgrade:\ncrds: CreateReplace\nremediation:\nretries: 3\nvalues:\neula:\naccept: true                                          # (1)\ncompany: Truxnell                                     # (2)\nemail: Truxnell@users.noreply.github.com              # (3)\nglobal:\npersistence:                                          # (4)\nstorageClass: ceph-block\nauth:                                                   # (5)\ntokenAuth:\nenabled: true\nclusterName: hegira                                     # (6)\ningress:\ncreate: true                                          # (9)\nhost: k10.${EXTERNAL_DOMAIN}\nannotations:\nkubernetes.io/ingress.class: traefik\ntraefik.ingress.kubernetes.io/router.entrypoints: websecure\ntraefik.ingress.kubernetes.io/router.middlewares: network-system-rfc1918-ips@kubernetescrd\nhajimari.io/enable: \"true\"\nhajimari.io/icon: backup-restore\nhajimari.io/appName: \"K10\"\nhajimari.io/url: \"https://k10.${EXTERNAL_DOMAIN}/k10/\"\nurlPath: \"k10\"                                        # (7)\nhosts:\n- k10.${EXTERNAL_DOMAIN}\ntls:\nenabled: true\ngrafana:                                                # (8)\nenabled: false\n</code></pre> <ol> <li>EULA must be 'manually' accepted</li> <li>'Company name' - required</li> <li>Email for license - github no-reply email is useful here - required</li> <li>Note we aren't using a existing PVC and letting ceph do as it pleases here - as we have a disaster recovery backup its not important to control this</li> <li>Tokenauth creates a token to login to k10 - helpful as its a easy login method</li> <li>Name of cluster</li> <li>Note this url path - it is a subpath - so my k10 is located at <code>https://k10.hegira.domain.tld/k10</code></li> <li>Grafana can be installed, but I want to use my central instance not k10's.</li> <li>Worth noting this key is required for ingress</li> </ol>"},{"location":"apps/k10/#disaster-recovery-key","title":"Disaster recovery key","text":"<p>k10 Disaster recovery can be enabled in settings -&gt; K10 Disaster Recovery in the UI.  When you enabled this, you will be asked for a passphrase, and given a cluster id.</p> <p>Doing a full DR from a clean install requires the passphrase placed in a secret before install.  I chose to put this in a SOPS secret which is loaded with the k10 helmrelease, so it is always on my cluster ready to go.  I also put the clusterID in the secret - k10 does not need it, but its there ready to go in a easy to find spot.</p> <p>You can find this clusterId if you have access to the remote DR backup folder - the folder is named after the clusterID.</p> <pre><code>apiVersion: v1\nkind: Secret\ntype: Opaque\nmetadata:\nname: k10-dr-secret\nnamespace: kasten-io\nstringData:\nkey: &lt;super secret password&gt;        # (1)\nclusterId: &lt;cluster id&gt;             # (2)\n</code></pre> <ol> <li>Password given at setup of DR - it is used as the encryption key for the DR.</li> <li>ClusterId given in the UI.  Looks like 09832d646-209d-438c-95bc-0fgfa5ac6d93</li> </ol>"},{"location":"apps/k10/#prometheus-federation","title":"Prometheus federation","text":"<p>k10's prometheus can be federated back to a main prometheus, using a ServiceMonitor.  The below is a example configuration that will scape the k10 prometheus.  k8s/manifests/kasten-io/k10/k10-config/  Has both the ServiceMonitor and a AlertManager.</p> <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\nname: k10\nnamespace: kasten-io\nspec:\nnamespaceSelector:\nmatchNames:\n- kasten-io\nselector:\nmatchLabels:\napp: prometheus\nendpoints:\n- port: http\nscheme: http\npath: /k10/prometheus/federate\nhonorLabels: true\ninterval: 15s\nparams:\n\"match[]\":\n- '{__name__=~\"jobs.*\"}'\n- '{__name__=~\"catalog.*\"}'\n</code></pre>"},{"location":"apps/k10/#login","title":"Login","text":"<p>Read the docs</p> <p>The k10 docs provide what is needed here https://docs.kasten.io/latest/access/authentication.html#token-authentication</p> <p>I chose to be lazy and just use the token auth that is provided.  I believe others use SSO solutions, feel free to choose your approach (and effort)</p> <pre><code>#Assume K10 is installed in the 'kasten-io' namespace\n#Extracting token from SA 'my-kasten-sa'\n#get the SA secret\nsa_secret=$(kubectl get serviceaccount my-kasten-sa -o jsonpath=\"{.secrets[0].name}\" --namespace kasten-io)\n#extract token\nkubectl get secret $sa_secret --namespace kasten-io -o jsonpath=\"{.data.token}{'\\n'}\" | base64 --decode\n</code></pre> <p>The key output to the console can just be pasted in as the password in the UI.</p>"},{"location":"apps/k10/#restoring-a-pvc","title":"Restoring a pvc","text":"<p>The general workflow for a gitops cluster can be described as:</p> <ol> <li>Scale down relevant deployments to 0 (To release the pvc's)</li> <li>In k10 UI, select the desired backup in the Applications section</li> <li>Select 'Data-only restore' (this only restores the pvc data and no configuration)</li> <li>Check progress of restore in dashboard</li> <li>Scale up deployment</li> </ol>"},{"location":"apps/k10/#disaster-recovery","title":"Disaster recovery","text":"<p>Learn from my mistakes</p> <p>Im writing this because I force-deleted a kustomization, which led to my entire cluster being wiped.  k10 saved my rear end and the DR worked fine.  Be careful deleting kustomizations!</p> <p>Read the docs</p> <p>The k10 docs provide what is needed here https://docs.kasten.io/latest/operating/dr.html</p> <p>As long as a DR policy is activated, the passphrase and clusterId has been retained - recovery is easy and safe.</p> <p>To recover, I bootstrap the cluster as fresh and let everything come up.  This brings up everything, as well as a blank k10.  This also installs the required secret <code>k10-dr-secret</code> as noted in 'Specifying a DR Passphrase' in the above link.</p> <p>Then we can run the disaster recovery helm install, checking and replacing in below the namespace, clusterID and profile name (likely <code>k10-disaster-recovery-policy</code>)</p> <pre><code>Install the helm chart that creates the K10 restore job and wait for completion of the `k10-restore` job\nAssumes that K10 is installed in 'kasten-io' namespace.\nhelm install k10-restore kasten/k10restore --namespace=kasten-io \\\n    --set sourceClusterID=&lt;source-clusterID&gt; \\\n    --set profile.name=&lt;location-profile-name&gt;\n</code></pre> <p>Ensure k10 is up to date</p> <p>Ensure k10 is up to date - having a older version and then installing the latest DR helm can lead to funky, yet undesired results.</p> <p>This will take a few minutes or more and reboot most of k10 at least once.  The UI will display a 'recovery in progress' and finally a 'complete' message</p> <p>Cluster id changes</p> <p>It appears my clusterID has changed after a successful restore, may be worth looking at after a successful DR.  I have had to update my clusterId to reflect the 'new' cluster</p>"},{"location":"cluster/bootstrap-workflow/","title":"Bootstrap Workflow","text":""},{"location":"cluster/bootstrap-workflow/#bootstrap-flow","title":"Bootstrap flow","text":"<p>The folder-structure doc still makes it a bit tricky to see the dependancies and workflow for the cluster.  Ill attempt to clarify that a bit with a confusing speghetti flowchart.</p> <p>* Steps require reconciliation of prior step</p> <p>In the deployment subgraph, each step uses <code>`dependsOn</code> in the yaml to require the previous step fully reconciled before it proceeds.</p> <p>In this way, there is a layering where the cluster will load the repositories, flux managment layer, variables, and finally orchestration to start installing apps in order.</p> <pre><code>    flowchart TB\n        ka[kubectl apply] --&gt; dc[deploy-cluster.yaml]\n        subgraph deploy\n            cr --&gt; mf[\"deploy/manage-flux.yaml*\"]\n            dc --&gt; cr[\"deploy/cluster-repositories.yaml\"*]\n            mf --&gt; cc[\"deploy/cluster-config.yaml*\"]\n            cc --&gt; co[\"deploy/cluster-orchestration*\"]\n        end\n        subgraph cluster-repositories\n            direction LR\n            cgr[Cluster Git Repositories]\n            chr[Cluster Helm Repositories]\n        end\n        subgraph global-repositories\n            direction LR\n            ggr[Global Git Repositories]\n            ghr[Global Helm Repositories]\n        end\n        cluster-repositories --&gt; global-repositories\n        dc --&gt; cluster-repositories\n        subgraph cluster-vars\n            direction LR\n            cf[Cluster Config]\n            cs[Cluster Secrets]\n        end\n        subgraph global-vars\n            gf[Global Config]\n            gs[Global Secrets]\n        end\n        cc --&gt; cluster-vars\n        cluster-vars --&gt; global-vars\n        subgraph orchestration\n            direction LR\n            cm[Cert Manager]\n            db[Databases]\n            df[Downloads]\n            ga[Games]\n            ha[Home-Automation]\n            ks[Kasten-io]\n        end\n        co --&gt; cm\n        co --&gt; db\n        co --&gt; df\n        co --&gt; ga\n        co --&gt; ha\n        co --&gt; ks\n        subgraph manifests\n            mcm[Cert Manager yaml]\n            mdb[Database yaml]\n            mdf[Downloads yaml]\n            mga[Games yaml]\n            mha[Home-Automation yaml]\n            mks[Kasten-io yaml]\n        end\n        cm --&gt; mcm\n        db --&gt; mdb\n        df --&gt; mdf\n        ga --&gt; mga\n        ha --&gt; mha\n        ks --&gt; mks</code></pre>"},{"location":"diagrams/cluster-map/","title":"Cluster Map","text":"<p>A cluster map of my network, using diagram.py plugin for mkdocs</p> <p></p>"},{"location":"diagrams/network-map/","title":"Network Map","text":"<p>A diagram map of my network, using diagram.py plugin for mkdocs</p> <p></p>"},{"location":"general/hardware/","title":"Hardware","text":"<p>The below lists the hardware and some specs of my homelab &amp; network infrastructure</p> Device OS Disk Size Data Disk Size Ram Operating System Name Purpose Raspberry Pi 4 64GB microSD N/A 4GB Talos technocore Sidero Master Raspberry Pi 4 120gb USB SSD N/A 8GB Talos leela Cluster (Hegira) Master Intel NUC8i5BEH Samsung SSD 850 500gb 1TB Crucial P1 NVMe 48GB Talos xerxes Cluster (Hegira) Worker Intel NUC6i7KYK 240GB WD Green SN350 NVMe 1TB Crucial P1 NVMe 48GB Talos SHODAN Cluster (Hegira) Worker Intel NUC11PAHi7 250GB Samsung SSD 840 1TB Kingston SNVS 1000G 64GB Talos tycho Cluster (Hegira) Worker Synology DS1513+ N/A 2 x 8TB WD Red &amp; 3 x 6TB WD Red 2GB Synology DSM hyperion NAS Raspberry Pi 4 64GB microSD N/A 2GB Arch (piKVM) piKVM piKVM UDM-Pro N/A N/A 4GB Unifi OS UDM-PRO Router USW-24-POE N/A N/A N/A Unifi OS N/A POE Switch U3 x AP-AC-Lite N/A N/A N/A Unifi OS N/A Wifi AP Nanobeam Gen2 5AC N/A N/A N/a Unifi OS N/A Wireless uplink"},{"location":"helm/creating-helm-chart/","title":"Creating a helm chart","text":"<p>Read docs at k8s-at-home.</p>"},{"location":"helm/creating-helm-chart/#template-chart-to-stdin","title":"Template chart to stdin","text":"<p>Outputs the rendered chart, useful to test it as you make changes</p> <pre><code>helm template nginx-php . --values ./values.yaml --debug\n</code></pre>"},{"location":"helm/creating-helm-chart/#install-from-local-folder","title":"Install from local folder","text":"<p>For <code>nginx-php</code> chart for example. <pre><code>helm install nginx-php nginx-php/ --values nginx-php/values.yaml\n</code></pre></p>"},{"location":"repository/folder-structure/","title":"Repository strucutre","text":"<p>Folder structures are important in many projects, and a repostirory for flux (or just k8s yaml) is no exception.</p> <p>Many folder structures exist and this is the second I have used - and likely not the last. The way my repo is layed out is not my own design - it is blatantly stolen from members of the k8s@home community.</p> <p>This folder structure inspired by</p> <ul> <li>bjw-s repo</li> <li>carpenike's repo</li> <li>xunholy's repo</li> </ul>"},{"location":"repository/folder-structure/#general-folder-structure","title":"General folder structure","text":"<p>The <code>k8s</code> folder is blown out and described in the next heading.</p> <pre><code>.\n\u251c\u2500\u2500 .github/                   # all github related files\n\u2502   \u251c\u2500\u2500 actions                # github actions\n\u2502   \u251c\u2500\u2500 lint                   # linter settings\n\u2502   \u251c\u2500\u2500 mkdocs                 # mkdocs settings\n\u2502   \u251c\u2500\u2500 scripts                # shell scripts\n\u2502   \u2514\u2500\u2500 workflows              # github workflows\n\u251c\u2500\u2500 .taskfiles/                # taskfiles (https://taskfile.dev)\n\u2502   \u2514\u2500\u2500 &lt;taskfile&gt;.yaml        # various taskfiles for faster/easier k8s managment\n\u251c\u2500\u2500 .vscode/                   # vscode files\n\u2502   \u251c\u2500\u2500 extensions.json        # recommended repository extensions\n\u2502   \u251c\u2500\u2500 settings.json          # repositroy settings (mainly favorites)\n\u2502   \u2514\u2500\u2500 yaml.code-snippets     # yaml snippets i use from time to time\n\u251c\u2500\u2500 docs/                      # mkdocs folder (markdown for this site)\n\u2502   \u251c\u2500\u2500 _includes              # markdown includes (abbreviations)\n\u2502   \u251c\u2500\u2500 _static                # static assets (css, js, etc)\n\u2502   \u251c\u2500\u2500 .draft                 # dump folde for mini-drafts or notes for docs\n\u2502   \u2514\u2500\u2500 src/                   # root of this site\n\u2502       \u251c\u2500\u2500 &lt;topic_heading&gt;/   # folder named for a topic\n\u2502       \u2502   \u2514\u2500\u2500 &lt;topics&gt;.md    # individual markdown files for this topic\n\u2502       \u251c\u2500\u2500 index.md           # landing page\n\u2502       \u2514\u2500\u2500 CNAME              # CNAME record for this site - required for custom domain on github pages\n\u251c\u2500\u2500 k8s/                       # All flux/k8s yaml folder\n\u2502   \u2514\u2500\u2500  (continued below)\n\u251c\u2500\u2500 tools/                     # Assorted deployment yaml for a few cluster tools\n\u2502   \u2514\u2500\u2500 &lt;tool&gt;.yaml            # tool deployment yaml\n\u251c\u2500\u2500 .gitignore                 # file list to be ignored by git\n\u251c\u2500\u2500 .pre-commit-config.yaml    # git precommit settings\n\u251c\u2500\u2500 .sops.yaml                 # sops settings\n\u2514\u2500\u2500 taskfile.yaml              # tasfile setup (points to .taskfile folder)\n</code></pre>"},{"location":"repository/folder-structure/#k8s-folder-structure-cluster-flux-bootstrap","title":"K8s folder structure - Cluster &amp; Flux bootstrap","text":"<p>This is the 'main' folder for the entire cluster, hence its own section.  The below outlines all the folders that enable the bootstrap of the cluseter, flux installation, loading of repositories and global/cluster variables&amp;secrets.</p> <p>Manifests (all the apps to run the cluster) will be, again, listed seperatly below.</p> <p>Still confused?</p> <p>Still confused by this?  The bootstrap page doc may help you visualize how this hangs together.  Perhaps.</p> <pre><code>.\n\u2514\u2500\u2500 k8s/                                            # (1)\n\u251c\u2500\u2500 charts/                                     # Extra charts\n\u2502   \u2514\u2500\u2500 kah-common-chart                        # Clone of k8s common chart\n\u251c\u2500\u2500 clusters/                                   # All cluster-specific yaml\n\u2502   \u251c\u2500\u2500 hegira/                                 # Prod cluster yaml\n\u2502   \u2502   \u251c\u2500\u2500 flux/                               # Flux yaml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 deploy/                         # Deployment-specific yaml\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 cluster-config.yaml         # kustomize to apply '../vars' folder\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 cluster-orcestration.yaml   # kustomize to apply '../orchestration' folder\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 cluster-repositories.yaml   # kustomize to apply '../repositories' folder\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 manage-flux.yaml            # kustomize to apply flux from its gitrepo\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 orchestration/                  # app orchestration yaml\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 &lt;namespace&gt;.yaml            # yaml per namespace, refers to 'k8s/manifests/&lt;namespace&gt;/&lt;app&gt;' folders\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 repositiores/                   # cluster-specific repositories\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 git                         # git repositories (namely this one)\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 helm                        # cluster-specific helm repos\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 vars/                           # cluster-specific variables\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 cluster-config.yaml         # cluster specific open vars\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 cluster-secrets.sops.yaml   # cluster-specific secret vars\n\u2502   \u2502   \u2514\u2500\u2500 deploy-cluster.yaml                 # cluster bootstrap to be applied manually.\n\u2502   \u251c\u2500\u2500 integrations/                           # folder for extra manifests\n\u2502   \u2502   \u2514\u2500\u2500 cilium-quick-install/               # minimal deployment of cilium to get cluster running\n\u2502   \u2514\u2500\u2500 sidero/                                 # sidero cluster\n\u2502       \u2514\u2500\u2500 &lt;same folder structure as hegira&gt;   # same structure as above\n\u251c\u2500\u2500 global/                                     # global resources for all clusters\n\u2502   \u251c\u2500\u2500 repositories/                           # global repositories\n\u2502   \u2502   \u251c\u2500\u2500 git/                                # global git repos\n\u2502   \u2502   \u2514\u2500\u2500 helm/                               # global helms\n\u2502   \u2514\u2500\u2500 vars/                                   # global vars\n\u2502       \u251c\u2500\u2500 global-config.yaml                  # global open vars\n\u2502       \u2514\u2500\u2500 global-secrets.sops.yaml            # global secrets\n\u2514\u2500\u2500 manifests/                                  # app manifests\n\u2514\u2500\u2500 &lt;namespace folders&gt;                     # folders per namespace\n\u2514\u2500\u2500 &lt;app folder&gt;                        # app folder\n\u2514\u2500\u2500 &lt;app yaml&gt;                      # app yaml\n</code></pre> <ol> <li>All k8s related yaml</li> </ol> <p>We can see here the key differences between clusters will be:</p> <ul> <li>Their cluster specific variables</li> <li>Their orchestration folder, which will call specific portions of the global manifest folder</li> </ul> <p>Its possible to also have differing cluster repositories, but with this being a mono-repository that is unlikely</p> <p>Why 'hegira' for the cluster name?</p> <p>While many use generic names for servers &amp; clusters, aligning to the 'servers are not pets' doctorine that tools like k8's bring, I still need to get my nerd on with a personal cluster.</p> <p>I got the idea for Hegira from Dan Simmons Hyperion Cantos.  It ties into the naming of the sidero cluster (technocore) and in general the theme of AI &amp; Humanity that is portraied in the books</p> <p>Hegira is also defined as a journey especially when undertaken to escape from a dangerous or undesirable situation, a somewhat medieval Latin translation of an Arabic word that can be translated to 'migration' or 'exodus', which actually somewhat fits the journey my homelab has taken over the years.</p>"},{"location":"repository/folder-structure/#k8s-folder-structure-app-manifests","title":"K8s folder structure - App Manifests","text":"<p>This lists the manifest folder, which contains all the namespace declerations and everything to run the apps for the cluster.</p> <p>Note this is stored globally, and the two clusters call on specific folders in this manifest to install the apps.  This is just my approach to lean toward a DRY (Dont Repeat Yourself) paradigm.</p> <p>This enables multiple clusters to re-use the exact same manifests - often using cluster-specific variables to ensure the app is installed in a cluster-specific manner.</p> <p>Not every folder is listed</p> <p>I havent added every folder in here or apps in folder.  I'm only breaking out important/unique ones for explanation. Most apps follow the below format, for example the downloads folder/namespace</p> <pre><code>.\n\u2514\u2500\u2500 downloads/                      # (1)\n\u251c\u2500\u2500 qbittorrent/                # (2)\n\u2502   \u251c\u2500\u2500 config-pvc.yaml         # (3)\n\u2502   \u251c\u2500\u2500 helmrelease.yaml       # (4)\n\u2502   \u2514\u2500\u2500 kustomization.yaml      # (5)\n\u251c\u2500\u2500 sabnzbd/                    # (8)\n\u2502   \u251c\u2500\u2500 config-pvc.yaml         # (9)\n\u2502   \u251c\u2500\u2500 helmrelease.yaml       # (10)\n\u2502   \u2514\u2500\u2500 kustomization.yaml      # (11)\n\u251c\u2500\u2500 namespace.yaml              # (6)\n\u2514\u2500\u2500 kustomization.yaml          # (7)\n</code></pre> <ol> <li>Folder for apps, same as namespace name</li> <li>App folder</li> <li>App persistent volume claim - usually named <code>app_name-config-v1</code>.  The version control <code>v1</code> allows easy upgrade to pvc's if we want to change sizing.</li> <li>Helm release for app</li> <li>Kustomization, calling the config-pvc.yaml and helmrelease.yaml</li> <li>Namespace resource file</li> <li>Kustomize for namespace only.  Note this file, unlike other repos, does not call upon the apps - it only refers to and creates the namespace.</li> <li>App folder</li> <li>App persistent volume claim - usually named <code>app_name-config-v1</code>.  The version control <code>v1</code> allows easy upgrade to pvc's if we want to change sizing.</li> <li>Helm release for app</li> <li>Kustomization, calling the config-pvc.yaml and helmrelease.yaml</li> </ol> <p>Why declare PVC's independantly</p> <p>We dont rely on the helm chart to generate a PVC for us - we prefer to declare them seperately and definitively. With the normal persistent volume policy of <code>Delete</code> in most persistant storage configs, anytime the PVC is removed the storage system will delete the PV - and any data it contains!  So if you make changes to the PVC or delete the helm release - its possible you will lose your data.  Specifying a PVC seperatly helps proect agains this.</p> <p>It also allows us to version control the pvc, which Ii useful if we need to change pvc size later.</p> <pre><code>.\n\u2514\u2500\u2500 manifests/                      # folder for all namespaces\n    \u251c\u2500\u2500 cert-manager/               # cert-manager folder\n    \u2502   \u251c\u2500\u2500 base/                   # helm-release for\n    \u2502   \u251c\u2500\u2500 certificate/            # cert-manager certificate requests\n    \u2502   \u2514\u2500\u2500 config/                 # cert-manager config\n    \u2502       \u251c\u2500\u2500 clusterissuer.yaml  # cert-manaager clusterissuer (letsencrypt config)\n    \u2502       \u251c\u2500\u2500 prometheusrule.yaml # prometheus monitoring rule for certs\n    \u2502       \u2514\u2500\u2500 secret.sops.yaml    # cloudflare api-token secret\n    \u251c\u2500\u2500 databases                   # namespace for databases (postgresql, maraidb)\n    \u251c\u2500\u2500 default/                    # namespace for sidero setup\n    \u2502   \u251c\u2500\u2500 clusters/               # sidero cluster yaml\n    \u2502   \u251c\u2500\u2500 environments/           # sidero environment yaml\n    \u2502   \u251c\u2500\u2500 serverclasses/          # sidero serverclass yaml\n    \u2502   \u2514\u2500\u2500 servers/                # sidero server yaml\n    \u251c\u2500\u2500 downloads                   # downloaders (sabnzbd, qbittorrent, etc)\n    \u251c\u2500\u2500 flux-system/                # flux-system add-ons\n    \u2502   \u251c\u2500\u2500 monitoring/             # prometheus rule &amp; podmonitor for monitoring flux\n    \u2502   \u251c\u2500\u2500 notifications/          # discord &amp; github notifications\n    \u2502   \u2514\u2500\u2500 webhook/                # github webhook to call flux on github push for instant reconciliation\n    \u251c\u2500\u2500 games                       # game servers (factorio, foundrytts)\n    \u251c\u2500\u2500 home-automation             # home automation (hass, nodered, mqtt)\n    \u251c\u2500\u2500 kasten-io/                  # kasten-io backup solution\n    \u2502   \u2514\u2500\u2500 k10/\n    \u2502       \u251c\u2500\u2500 blueprints/         # k10 blueprint yaml (extensions of backup method)\n    \u2502       \u251c\u2500\u2500 monitoring/         # prometheus monitoring rules\n    \u2502       \u251c\u2500\u2500 policies/           # k10 policies (backup schedules)\n    \u2502       \u2514\u2500\u2500 profiles/           # k10 profiles (backup locations)\n    \u251c\u2500\u2500 kube-system                 # extra apps for cluster operatorions (CNI, plugins, housekeeping)\n        \u251c\u2500\u2500 cillium                 # this manifest is applied over the top of the quick-install bootstrap\n    \u251c\u2500\u2500 media                       # media players/servers (Plex, dizquetv)\n    \u251c\u2500\u2500 network-system              # network apps/ingress (metallb, traefik, externaldns)\n    \u251c\u2500\u2500 organizarrs                 # organizer apps (*arr's)\n    \u251c\u2500\u2500 rook-ceph/                  # rook-ceph (distributed storage system)\n    \u2502   \u251c\u2500\u2500 add-ons/                # rook-ceph addons (grafana dashboards)\n    \u2502   \u251c\u2500\u2500 cluster/                # rook-ceph cluster definition\n    \u2502   \u251c\u2500\u2500 operator/               # rook-ceph operator\n    \u2502   \u2514\u2500\u2500 snapshot-controller/    # snapshot controller addon (for kasten snapshots)\n    \u251c\u2500\u2500 security                    # security apps (vaultwarden, SSO)\n    \u251c\u2500\u2500 services                    # general service apps (hajimari, joplin-server, syncthing)\n    \u251c\u2500\u2500 sidero-system               # sidero system apps (arm64 patch, loadbalancer)\n    \u251c\u2500\u2500 system-monitoring           # system monitoring (prometheus, grafana, botkube)\n    \u2514\u2500\u2500 vpn                         # vpn apps (pod gateway, used for qbittorrent/namespace routing)\n</code></pre>"},{"location":"repository/semantic-git-messages/","title":"Semantic Commit Messages","text":"<p>In this repo, I have made some effort to use semantic commit messaging to be a bit more descriptive/declaritive of what my changes do. Despite being a personal project, it still aids in readability of your own changes, and helps track why and how changes are made in a glancable format.</p> <p>Examples:</p> <ul> <li><code>fix(images): update helm values ghcr.io/k8s-at-home/prowlarr to v0.2.0.1560</code></li> <li><code>docs: draft mullvad images</code></li> <li><code>feat: enable qbittorrent</code></li> <li><code>fix: tpyo</code></li> </ul> <p>Below is sourced form https://gist.github.com/joshbuchea/6f47e86d2510bce28f8e7f42ae84c716 with some minor amendmends</p> <p>Info</p> <p>The best source for this is https://www.conventionalcommits.org/</p>"},{"location":"repository/semantic-git-messages/#semantic-commit-rundown","title":"Semantic commit rundown","text":"<p>See how a minor change to your commit message style can make you a better programmer.</p> <p>Format: <code>&lt;type&gt;(&lt;scope&gt;): &lt;subject&gt;</code></p> <p><code>&lt;scope&gt;</code> is optional.  ! after scope/type indicates breaking change.  This may be found in renovatebot commits mainly.</p>"},{"location":"repository/semantic-git-messages/#example","title":"Example","text":"<pre><code>feat: add hat wobble\n^--^  ^------------^\n|     |\n|     +-&gt; Summary in present tense.\n|\n+-------&gt; Type: chore, docs, feat, fix, refactor, style, or test.\n</code></pre> <p>More Examples:</p> <ul> <li><code>feat</code>: (new feature for the user, not a new feature for build script)</li> <li><code>fix</code>: (bug fix for the user, not a fix to a build script)</li> <li><code>docs</code>: (changes to the documentation)</li> <li><code>style</code>: (formatting, missing semi colons, etc; no production code change)</li> <li><code>refactor</code>: (refactoring production code, eg. renaming a variable, not a bugfix or feature)</li> <li><code>test</code>: (adding missing tests, refactoring tests; no production code change)</li> <li><code>chore</code>: (updating grunt tasks etc; no production code change)</li> <li><code>ci</code>: (continuous integration tasks)</li> <li><code>build</code>: (build tasks)</li> <li><code>perf</code>: (changes that affect performance)</li> </ul> <p>References:</p> <ul> <li>https://www.conventionalcommits.org/</li> <li>https://seesparkbox.com/foundry/semantic_commit_messages</li> <li>http://karma-runner.github.io/1.0/dev/git-commit-msg.html</li> </ul>"},{"location":"sidero/retrieve_talosconfig/","title":"Retreiving lost talosconfig","text":"<p>For when you have <code>kubeconfig</code> / cluster access, but no talosconfig.</p>"},{"location":"sidero/retrieve_talosconfig/#retrieve-crtkey-admin-key-from-machineconfig","title":"Retrieve crt/key &amp; admin key from machineconfig.","text":"<p>Run a yaml such as tools/hostpath</p> tools/hostpath.yaml<pre><code>---\napiVersion: v1\nkind: Pod\nmetadata:\nname: ubuntu-debug\nnamespace: default\nspec:\ncontainers:\n- name: ubuntu\nimage: ubuntu\ncommand: [ \"sleep\", \"600000\" ]\nvolumeMounts:\n- mountPath: /hostRoot\nname: root\nreadOnly: true\nvolumes:\n- name: root\nhostPath:\npath: /\ntype: Directory\n</code></pre> <p>Run pod <pre><code>kubectl apply -f tools/hostpath.yaml\npod/ubuntu-debug configured\n</code></pre></p> <p>Inside pod:</p> <pre><code>cat hostRoot/system/state/config.yaml\n\nversion: v1alpha1 # Indicates the schema used to decode the contents.\ndebug: false # Enable verbose logging to the console.\npersist: true # Indicates whether to pull the machine config upon every boot.\n# Provides machine specific configuration options.\nmachine:\n    type: init # Defines the role of the machine within the cluster.\ntoken: ckqzyg.uzaqtz5777r97dkp # The `token` is used by a machine to join the PKI of the cluster.\n# The root certificate authority of the PKI.\nca:\n        crt: &lt;machine ca.crt&gt;\n        key: &lt;machine ca.key&gt;\n    # Extra certificate subject alternative names for the machine's certificate.\ncertSANs: []\n#   # Uncomment this to enable SANs.\n#   - 10.0.0.10\n#   - 172.16.0.10\n#   - 192.168.0.10\n...\n</code></pre>"},{"location":"sidero/sidero-install/","title":"Installing sidero","text":""},{"location":"sidero/sidero-install/#planning","title":"Planning","text":"<p>I would suggest you read most of the Sidero docs Sidero, as well as some of the Talos Introduction Docs before getting started. Familiarity with these will be important, the below is more of a fleshed out version of my docs and isn't intended to replace the real thing - especially as there will be drift.</p>"},{"location":"sidero/sidero-install/#hardware-considerations","title":"Hardware considerations","text":"<p>As Sidero runs in its own k8s cluster, you need to have an always on second cluster to run.  Sidero really only needs 4GB of RAM and is very light on CPU - making a SBC like the Raspberry Pi 4 4GB a perfect option for home use.  A small PC would also be fine if thats more to your linking.</p> <p>Ask yourself the following questions:</p> <ul> <li>How may masters/workers for my cluster.</li> <li>CPU arcitecture - will I use strinctly AMD64 or will I have some ARM64 SBC's?</li> <li>Will I replace flannel CNI with another?</li> <li>Will I use a VIP for HA control plane?</li> </ul> <p>Refer to Sidero documentation on Raspi-4 servers</p> <p>Info here created with reference to Sidero docs - Always read their latest docs!</p> <p>Ref: https://www.sidero.dev/docs/v0.5/guides/rpi4-as-servers/</p>"},{"location":"sidero/sidero-install/#software-considerations","title":"Software considerations","text":"<p>Refer to Sidero documentation on cli pre-requisites</p> <p>Ref: https://www.sidero.dev/docs/v0.5/getting-started/prereq-cli-tools/</p> <p>Grab the binaries specified:</p> <ul> <li><code>kubectl</code></li> <li><code>clusterctl</code></li> <li><code>talosctl</code></li> </ul> <p>Tip</p> <p><code>talosctl</code> can be used to stand up a local docker cluster.  This can be amazing for doing testing on sidero, or just local cluster testing in general.  More info here</p> <p>Warning</p> <p>Be careful on the verison of <code>clusterctl</code> you use.  The latest version may not be the one you need, and may not work with the api versions.  Refer to the Sidero Github as they have a compatibility table in the readme.</p> <ul> <li>Now to setup dhcp for dual booting.  If you do amd64 its not so bad as its one config, but with pi you need to config it to be able to send either arm64 or amd64 binaries.  https://www.sidero.dev/docs/v0.4/getting-started/prereq-dhcp/ is the document.  I had problems here, and i had to mod my udm-pro's dnsmasq with extra configs that get applied at boot to make it work. Codeblock on this page gives you enough info to mod your config - cant see your router on</li> <li>I used the bootstrapping section a few times to test - https://www.sidero.dev/docs/v0.4/guides/bootstrapping/.  Cool use case.</li> </ul>"},{"location":"sidero/sidero-install/#setup-dhcp-to-serve-pxe-boot-files","title":"Setup DHCP to serve PXE boot files","text":"<p>Refer to Sidero documentation on DHCP setup</p> <p>Ref: https://www.sidero.dev/docs/v0.5/getting-started/prereq-dhcp/</p> <p>This step will be specific to your network</p>"},{"location":"sidero/sidero-intro/","title":"Introduction to Sidero &amp; Talos","text":""},{"location":"sidero/sidero-intro/#talos","title":"Talos","text":"<p>Talos is a custom Linux OS designed to run Kubernetes and only Kubernetes.  The project aims to deliver a OS that is secure, immutable and minimal.  Talos is API driven and has no ssh, shell or interactive commands. It runs in memory via squashFS, and mounds the root filesystem as read only. It comes with Kubernetes (K8s) installed and ready for production use.</p> <p>This means a dramatic reduction in attack surface, reduced/eliminates configuration drift and delivers a robust OS for Kubernetes.</p> <p>Tldr</p> <p>Talos is a purpose-build OS for kubentes, and only Kubernetes.  Talos is pared back to only what is needed to run kurbenetes and run it well.  It runs in RAM, has no ssh or shell, and its entirely API driven.  Node configuration is applied via config files, eliminating config drift and need for complex ansible/terraform files.</p>"},{"location":"sidero/sidero-intro/#sidero","title":"Sidero","text":"<p>Sidero is a bare metal lifecycle manager for Talos using clusterAPI.  It runs in a managment cluster (i.e. seperate to your main cluster) and allows you to declaratively define your cluster and server requirements in YAML.  Once provisioned, nodes can be PXE booted from Sidero and a Talos config applied to the node - automating the format, install and config of your nodes.</p> <p>Tldr</p> <p>In short, it can allow nodes to be connected to the network, PXE boot to a installer, and then wipe/provision the node with TalosOS.  The node will then reboot, getting a configuration file from Sidero.  This will stand up a Master node, etc, and connect workers to your note.</p> <p>This can be a great alternative to lengthy Ansible/Terraform scripts, which often are taking a general purpose OS like Debian or Ubuntu and trying to strip it down for Kurbenetes.</p>"},{"location":"sidero/talos-dev-cluster/","title":"Local talos cluster in docker","text":""},{"location":"sidero/talos-update/","title":"Talos update","text":"<p>Talos updates are really simple:</p> <p>Simply change the version number at the end of the --image tag.</p> <pre><code>\u276f talosctl upgrade --nodes 10.8.20.40 \\\n--image ghcr.io/siderolabs/installer:v1.0.0-beta.0\nNODE         ACK                        STARTED\n10.8.20.40   Upgrade request received   2022-03-06 21:29:55.796035744 +1100 AEDT m=+4.542098674\n</code></pre> <p>After a successful upgrade, its recommended to then loop back on the sidero install yaml to bump the install version for if/when you need to install a new node/reset a existing node.</p>"}]}